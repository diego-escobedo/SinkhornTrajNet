/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from lib.growth_net import GrowthNet
from lib import utils
from lib.visualize_flow import visualize_transform
from lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from lib.viz_scrna import save_trajectory_density

from geomloss import SamplesLoss

# from train_misc import standard_normal_logprob
from train_misc import (
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_vanilla,
)

import dataset
from parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, logger, full_data, train_loss_fn):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.

    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """

    # Backward pass accumulating losses, previous state and deltas
    loss = torch.zeros(size=(1, 1)).to(device)
    z = None
    for i, (itp, tp) in enumerate(zip(args.int_tps[:-1], args.timepoints[:-1])): #dont integrate the last one obviously
        # tp counts down from last
        integration_times = torch.tensor([itp, itp + args.time_scale])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        if i != args.leaveout_timepoint:
            idx = args.data.sample_index(n="all", label_subset=tp) #used to be n=args.batch_size, want to use all data points 
            x = args.data.get_data()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
        else:
            x = z
        
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise

        # transform to next timepoint
        z = model(x, integration_times=integration_times)
        if args.timepoints[i+1] == args.leaveout_timepoint:
            ground_truth_ix = args.data.sample_index(n="all", label_subset=args.timepoints[i+1])
            ground_truth = args.data.get_data()[ground_truth_ix]
            gt = torch.from_numpy(ground_truth).type(torch.float32).to(device)
            loss += train_loss_fn(z, gt)

    return loss


def train(
    device, args, model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")

    end = time.time()
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        #step the input mapping...take care to account for the regularizedODE wrapper
        odefunc = model.chain[0].odefunc
        if len(regularization_coeffs) > 0:
            odefunc = odefunc.odefunc
        odefunc.diffeq.feature_mapping.step(itr / args.niters)

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, logger, full_data, train_loss_fn)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()



        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            with torch.no_grad():
                    visualize(device, args, model, itr)

        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }

            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_vanilla(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )

    if args.spectral_norm:
        add_spectral_norm(model)

    logger.info(model)
    n_param = count_parameters(model)
    logger.info("Number of trainable parameters: {}".format(n_param))

    train(
        device,
        args,
        model,
        regularization_coeffs,
        regularization_fns,
        logger,
    )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from lib.growth_net import GrowthNet
from lib import utils
from lib.visualize_flow import visualize_transform
from lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from lib.viz_scrna import save_trajectory_density

from geomloss import SamplesLoss

# from train_misc import standard_normal_logprob
from train_misc import (
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_vanilla,
)

import dataset
from parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, logger, full_data, train_loss_fn):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.

    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """

    # Backward pass accumulating losses, previous state and deltas
    loss = torch.zeros(size=(1, 1)).to(device)
    z = None
    for i, (itp, tp) in enumerate(zip(args.int_tps[:-1], args.timepoints[:-1])): #dont integrate the last one obviously
        # tp counts down from last
        integration_times = torch.tensor([itp, itp + args.time_scale])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        if i != args.leaveout_timepoint:
            idx = args.data.sample_index(n="all", label_subset=tp) #used to be n=args.batch_size, want to use all data points 
            x = args.data.get_data()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
        else:
            x = z
        
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise

        # transform to next timepoint
        z = model(x, integration_times=integration_times)
        if args.timepoints[i+1] == args.leaveout_timepoint:
            ground_truth_ix = args.data.sample_index(n="all", label_subset=args.timepoints[i+1])
            ground_truth = args.data.get_data()[ground_truth_ix]
            gt = torch.from_numpy(ground_truth).type(torch.float32).to(device)
            loss += train_loss_fn(z, gt)

    return loss


def train(
    device, args, model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")

    end = time.time()
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        #step the input mapping...take care to account for the regularizedODE wrapper
        odefunc = model.chain[0].odefunc
        if len(regularization_coeffs) > 0:
            odefunc = odefunc.odefunc
        odefunc.diffeq.feature_mapping.step(itr / args.niters)

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, logger, full_data, train_loss_fn)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()



        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            with torch.no_grad():
                    visualize(device, args, model, itr)

        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }

            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_vanilla(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )

    if args.spectral_norm:
        add_spectral_norm(model)

    logger.info(model)
    n_param = count_parameters(model)
    logger.info("Number of trainable parameters: {}".format(n_param))

    train(
        device,
        args,
        model,
        regularization_coeffs,
        regularization_fns,
        logger,
    )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from lib.growth_net import GrowthNet
from lib import utils
from lib.visualize_flow import visualize_transform
from lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from lib.viz_scrna import save_trajectory_density

from geomloss import SamplesLoss

# from train_misc import standard_normal_logprob
from train_misc import (
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_vanilla,
)

import dataset
from parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, logger, full_data, train_loss_fn):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.

    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """

    # Backward pass accumulating losses, previous state and deltas
    loss = torch.zeros(size=(1, 1)).to(device)
    z = None
    for i, (itp, tp) in enumerate(zip(args.int_tps[:-1], args.timepoints[:-1])): #dont integrate the last one obviously
        # tp counts down from last
        integration_times = torch.tensor([itp, itp + args.time_scale])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        if i != args.leaveout_timepoint:
            idx = args.data.sample_index(n="all", label_subset=tp) #used to be n=args.batch_size, want to use all data points 
            x = args.data.get_data()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
        else:
            x = z
        
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise

        # transform to next timepoint
        z = model(x, integration_times=integration_times)
        if args.timepoints[i+1] == args.leaveout_timepoint:
            ground_truth_ix = args.data.sample_index(n="all", label_subset=args.timepoints[i+1])
            ground_truth = args.data.get_data()[ground_truth_ix]
            gt = torch.from_numpy(ground_truth).type(torch.float32).to(device)
            loss += train_loss_fn(z, gt)

    return loss


def train(
    device, args, model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")

    end = time.time()
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        #step the input mapping...take care to account for the regularizedODE wrapper
        odefunc = model.chain[0].odefunc
        if len(regularization_coeffs) > 0:
            odefunc = odefunc.odefunc
        odefunc.diffeq.feature_mapping.step(itr / args.niters)

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, logger, full_data, train_loss_fn)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()



        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            with torch.no_grad():
                    visualize(device, args, model, itr)

        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }

            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_vanilla(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )

    if args.spectral_norm:
        add_spectral_norm(model)

    logger.info(model)
    n_param = count_parameters(model)
    logger.info("Number of trainable parameters: {}".format(n_param))

    train(
        device,
        args,
        model,
        regularization_coeffs,
        regularization_fns,
        logger,
    )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): RegularizedVanillaODEfunc(
        (odefunc): VanillaODEfunc(
          (diffeq): DiffeqNet(
            (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
            (feature_mapping): InputMapping(
              (B): Linear(in_features=6, out_features=262, bias=False)
            )
            (fc1): Linear(in_features=262, out_features=256, bias=True)
            (fc2): Linear(in_features=256, out_features=256, bias=True)
            (fc3): Linear(in_features=256, out_features=5, bias=True)
            (nonl): Tanh()
          )
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from lib.growth_net import GrowthNet
from lib import utils
from lib.visualize_flow import visualize_transform
from lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from lib.viz_scrna import save_trajectory_density

from geomloss import SamplesLoss

# from train_misc import standard_normal_logprob
from train_misc import (
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_vanilla,
)

import dataset
from parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, logger, full_data, train_loss_fn):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.

    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """

    # Backward pass accumulating losses, previous state and deltas
    loss = torch.zeros(size=(1, 1)).to(device)
    z = None
    for i, (itp, tp) in enumerate(zip(args.int_tps[:-1], args.timepoints[:-1])): #dont integrate the last one obviously
        # tp counts down from last
        integration_times = torch.tensor([itp, itp + args.time_scale])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        if i != args.leaveout_timepoint:
            idx = args.data.sample_index(n="all", label_subset=tp) #used to be n=args.batch_size, want to use all data points 
            x = args.data.get_data()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
        else:
            x = z
        
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise

        # transform to next timepoint
        z = model(x, integration_times=integration_times)
        if args.timepoints[i+1] == args.leaveout_timepoint:
            ground_truth_ix = args.data.sample_index(n="all", label_subset=args.timepoints[i+1])
            ground_truth = args.data.get_data()[ground_truth_ix]
            gt = torch.from_numpy(ground_truth).type(torch.float32).to(device)
            loss += train_loss_fn(z, gt)

    return loss


def train(
    device, args, model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")

    end = time.time()
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        #step the input mapping...take care to account for the regularizedODE wrapper
        odefunc = model.chain[0].odefunc
        if len(regularization_coeffs) > 0:
            odefunc = odefunc.odefunc
        odefunc.diffeq.feature_mapping.step(itr / args.niters)

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, logger, full_data, train_loss_fn)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()



        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            with torch.no_grad():
                    visualize(device, args, model, itr)

        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }

            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_vanilla(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )

    if args.spectral_norm:
        add_spectral_norm(model)

    logger.info(model)
    n_param = count_parameters(model)
    logger.info("Number of trainable parameters: {}".format(n_param))

    train(
        device,
        args,
        model,
        regularization_coeffs,
        regularization_fns,
        logger,
    )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): RegularizedVanillaODEfunc(
        (odefunc): VanillaODEfunc(
          (diffeq): DiffeqNet(
            (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
            (feature_mapping): InputMapping(
              (B): Linear(in_features=6, out_features=262, bias=False)
            )
            (fc1): Linear(in_features=262, out_features=256, bias=True)
            (fc2): Linear(in_features=256, out_features=256, bias=True)
            (fc3): Linear(in_features=256, out_features=5, bias=True)
            (nonl): Tanh()
          )
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from lib.growth_net import GrowthNet
from lib import utils
from lib.visualize_flow import visualize_transform
from lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from lib.viz_scrna import save_trajectory_density

from geomloss import SamplesLoss

# from train_misc import standard_normal_logprob
from train_misc import (
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_vanilla,
)

import dataset
from parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, logger, full_data, train_loss_fn):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.

    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """

    # Backward pass accumulating losses, previous state and deltas
    loss = torch.zeros(size=(1, 1)).to(device)
    z = None
    for i, (itp, tp) in enumerate(zip(args.int_tps[:-1], args.timepoints[:-1])): #dont integrate the last one obviously
        # tp counts down from last
        integration_times = torch.tensor([itp, itp + args.time_scale])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        if i != args.leaveout_timepoint:
            idx = args.data.sample_index(n="all", label_subset=tp) #used to be n=args.batch_size, want to use all data points 
            x = args.data.get_data()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
        else:
            x = z
        
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise

        # transform to next timepoint
        z = model(x, integration_times=integration_times)
        if args.timepoints[i+1] == args.leaveout_timepoint:
            ground_truth_ix = args.data.sample_index(n="all", label_subset=args.timepoints[i+1])
            ground_truth = args.data.get_data()[ground_truth_ix]
            gt = torch.from_numpy(ground_truth).type(torch.float32).to(device)
            loss += train_loss_fn(z, gt)

    return loss


def train(
    device, args, model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")

    end = time.time()
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        #step the input mapping...take care to account for the regularizedODE wrapper
        odefunc = model.chain[0].odefunc
        if len(regularization_coeffs) > 0:
            odefunc = odefunc.odefunc
        odefunc.diffeq.feature_mapping.step(itr / args.niters)

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, logger, full_data, train_loss_fn)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()



        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        print("this tha log msg", log_message)
        raise Error
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            with torch.no_grad():
                    visualize(device, args, model, itr)

        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }

            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_vanilla(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )

    if args.spectral_norm:
        add_spectral_norm(model)

    logger.info(model)
    n_param = count_parameters(model)
    logger.info("Number of trainable parameters: {}".format(n_param))

    train(
        device,
        args,
        model,
        regularization_coeffs,
        regularization_fns,
        logger,
    )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=262, bias=False)
          )
          (fc1): Linear(in_features=262, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from lib.growth_net import GrowthNet
from lib import utils
from lib.visualize_flow import visualize_transform
from lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from lib.viz_scrna import save_trajectory_density

from geomloss import SamplesLoss

# from train_misc import standard_normal_logprob
from train_misc import (
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_vanilla,
)

import dataset
from parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, logger, full_data, train_loss_fn):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.

    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """

    # Backward pass accumulating losses, previous state and deltas
    loss = torch.zeros(size=(1, 1)).to(device)
    z = None
    for i, (itp, tp) in enumerate(zip(args.int_tps[:-1], args.timepoints[:-1])): #dont integrate the last one obviously
        # tp counts down from last
        integration_times = torch.tensor([itp, itp + args.time_scale])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        if i != args.leaveout_timepoint:
            idx = args.data.sample_index(n="all", label_subset=tp) #used to be n=args.batch_size, want to use all data points 
            x = args.data.get_data()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
        else:
            x = z
        
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise

        # transform to next timepoint
        z = model(x, integration_times=integration_times)
        if args.timepoints[i+1] == args.leaveout_timepoint:
            ground_truth_ix = args.data.sample_index(n="all", label_subset=args.timepoints[i+1])
            ground_truth = args.data.get_data()[ground_truth_ix]
            gt = torch.from_numpy(ground_truth).type(torch.float32).to(device)
            loss += train_loss_fn(z, gt)

    return loss


def train(
    device, args, model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    print("this tha log msg")
    raise Error

    end = time.time()
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        #step the input mapping...take care to account for the regularizedODE wrapper
        odefunc = model.chain[0].odefunc
        if len(regularization_coeffs) > 0:
            odefunc = odefunc.odefunc
        odefunc.diffeq.feature_mapping.step(itr / args.niters)

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, logger, full_data, train_loss_fn)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()



        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            with torch.no_grad():
                    visualize(device, args, model, itr)

        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }

            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_vanilla(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )

    if args.spectral_norm:
        add_spectral_norm(model)

    logger.info(model)
    n_param = count_parameters(model)
    logger.info("Number of trainable parameters: {}".format(n_param))

    train(
        device,
        args,
        model,
        regularization_coeffs,
        regularization_fns,
        logger,
    )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=262, bias=False)
          )
          (fc1): Linear(in_features=262, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from lib.growth_net import GrowthNet
from lib import utils
from lib.visualize_flow import visualize_transform
from lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from lib.viz_scrna import save_trajectory_density

from geomloss import SamplesLoss

# from train_misc import standard_normal_logprob
from train_misc import (
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_vanilla,
)

import dataset
from parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, logger, full_data, train_loss_fn):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.

    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """

    # Backward pass accumulating losses, previous state and deltas
    loss = torch.zeros(size=(1, 1)).to(device)
    z = None
    for i, (itp, tp) in enumerate(zip(args.int_tps[:-1], args.timepoints[:-1])): #dont integrate the last one obviously
        # tp counts down from last
        integration_times = torch.tensor([itp, itp + args.time_scale])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        if i != args.leaveout_timepoint:
            idx = args.data.sample_index(n="all", label_subset=tp) #used to be n=args.batch_size, want to use all data points 
            x = args.data.get_data()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
        else:
            x = z
        
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise

        # transform to next timepoint
        z = model(x, integration_times=integration_times)
        if args.timepoints[i+1] == args.leaveout_timepoint:
            ground_truth_ix = args.data.sample_index(n="all", label_subset=args.timepoints[i+1])
            ground_truth = args.data.get_data()[ground_truth_ix]
            gt = torch.from_numpy(ground_truth).type(torch.float32).to(device)
            loss += train_loss_fn(z, gt)

    return loss


def train(
    device, args, model, regularization_coeffs, regularization_fns, logger
):
    print("this tha log msg")
    raise Error
    
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    

    end = time.time()
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        #step the input mapping...take care to account for the regularizedODE wrapper
        odefunc = model.chain[0].odefunc
        if len(regularization_coeffs) > 0:
            odefunc = odefunc.odefunc
        odefunc.diffeq.feature_mapping.step(itr / args.niters)

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, logger, full_data, train_loss_fn)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()



        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            with torch.no_grad():
                    visualize(device, args, model, itr)

        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }

            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_vanilla(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )

    if args.spectral_norm:
        add_spectral_norm(model)

    logger.info(model)
    n_param = count_parameters(model)
    logger.info("Number of trainable parameters: {}".format(n_param))

    train(
        device,
        args,
        model,
        regularization_coeffs,
        regularization_fns,
        logger,
    )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=262, bias=False)
          )
          (fc1): Linear(in_features=262, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from lib.growth_net import GrowthNet
from lib import utils
from lib.visualize_flow import visualize_transform
from lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from lib.viz_scrna import save_trajectory_density

from geomloss import SamplesLoss

# from train_misc import standard_normal_logprob
from train_misc import (
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_vanilla,
)

import dataset
from parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, logger, full_data, train_loss_fn):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.

    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """

    # Backward pass accumulating losses, previous state and deltas
    loss = torch.zeros(size=(1, 1)).to(device)
    z = None
    for i, (itp, tp) in enumerate(zip(args.int_tps[:-1], args.timepoints[:-1])): #dont integrate the last one obviously
        # tp counts down from last
        integration_times = torch.tensor([itp, itp + args.time_scale])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        if i != args.leaveout_timepoint:
            idx = args.data.sample_index(n="all", label_subset=tp) #used to be n=args.batch_size, want to use all data points 
            x = args.data.get_data()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
        else:
            x = z
        
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise

        # transform to next timepoint
        z = model(x, integration_times=integration_times)
        if args.timepoints[i+1] == args.leaveout_timepoint:
            ground_truth_ix = args.data.sample_index(n="all", label_subset=args.timepoints[i+1])
            ground_truth = args.data.get_data()[ground_truth_ix]
            gt = torch.from_numpy(ground_truth).type(torch.float32).to(device)
            loss += train_loss_fn(z, gt)

    return loss


def train(
    device, args, model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    

    end = time.time()
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        #step the input mapping...take care to account for the regularizedODE wrapper
        odefunc = model.chain[0].odefunc
        if len(regularization_coeffs) > 0:
            odefunc = odefunc.odefunc
        odefunc.diffeq.feature_mapping.step(itr / args.niters)

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, logger, full_data, train_loss_fn)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()



        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            with torch.no_grad():
                    visualize(device, args, model, itr)

        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }

            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)
    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )
    print("this tha log msg")
    raise Error
    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_vanilla(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )

    if args.spectral_norm:
        add_spectral_norm(model)

    logger.info(model)
    n_param = count_parameters(model)
    logger.info("Number of trainable parameters: {}".format(n_param))

    train(
        device,
        args,
        model,
        regularization_coeffs,
        regularization_fns,
        logger,
    )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
""" main.py

Learns ODE from scrna data

"""
import os
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time

import torch
import torch.nn.functional as F
import torch.optim as optim

from lib.growth_net import GrowthNet
from lib import utils
from lib.visualize_flow import visualize_transform
from lib.viz_scrna import (
    save_trajectory,
    trajectory_to_video,
    save_vectors,
)
from lib.viz_scrna import save_trajectory_density

from geomloss import SamplesLoss

# from train_misc import standard_normal_logprob
from train_misc import (
    count_nfe,
    count_parameters,
    count_total_time,
    add_spectral_norm,
    spectral_norm_power_iteration,
    create_regularization_fns,
    get_regularization,
    append_regularization_to_log,
    build_model_vanilla,
)

import dataset
from parse import parser

matplotlib.use("Agg")


def get_transforms(device, args, model, integration_times):
    """
    Given a list of integration points,
    returns a function giving integration times
    """

    def sample_fn(z, logpz=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times
        ]
        if logpz is not None:
            # TODO this works right?
            for it in int_list:
                z, logpz = model(z, logpz, integration_times=it, reverse=True)
            return z, logpz
        else:
            for it in int_list:
                z = model(z, integration_times=it, reverse=True)
            return z

    def density_fn(x, logpx=None):
        int_list = [
            torch.tensor([it - args.time_scale, it]).type(torch.float32).to(device)
            for it in integration_times[::-1]
        ]
        if logpx is not None:
            for it in int_list:
                x, logpx = model(x, logpx, integration_times=it, reverse=False)
            return x, logpx
        else:
            for it in int_list:
                x = model(x, integration_times=it, reverse=False)
            return x

    return sample_fn, density_fn


def compute_loss(device, args, model, logger, full_data, train_loss_fn):
    """
    Compute loss by integrating backwards from the last time step
    At each time step integrate back one time step, and concatenate that
    to samples of the empirical distribution at that previous timestep
    repeating over and over to calculate the likelihood of samples in
    later timepoints iteratively, making sure that the ODE is evaluated
    at every time step to calculate those later points.

    The growth model is a single model of time independent cell growth /
    death rate defined as a variation from uniform.
    """

    # Backward pass accumulating losses, previous state and deltas
    loss = torch.zeros(size=(1, 1)).to(device)
    z = None
    for i, (itp, tp) in enumerate(zip(args.int_tps[:-1], args.timepoints[:-1])): #dont integrate the last one obviously
        # tp counts down from last
        integration_times = torch.tensor([itp, itp + args.time_scale])
        integration_times = integration_times.type(torch.float32).to(device)
        # integration_times.requires_grad = True

        # load data and add noise
        if i != args.leaveout_timepoint:
            idx = args.data.sample_index(n="all", label_subset=tp) #used to be n=args.batch_size, want to use all data points 
            x = args.data.get_data()[idx]
            x = torch.from_numpy(x).type(torch.float32).to(device)
        else:
            x = z
        
        if args.training_noise > 0.0:
            x += np.random.randn(*x.shape) * args.training_noise

        # transform to next timepoint
        z = model(x, integration_times=integration_times)
        if args.timepoints[i+1] == args.leaveout_timepoint:
            ground_truth_ix = args.data.sample_index(n="all", label_subset=args.timepoints[i+1])
            ground_truth = args.data.get_data()[ground_truth_ix]
            gt = torch.from_numpy(ground_truth).type(torch.float32).to(device)
            loss += train_loss_fn(z, gt)

    return loss


def train(
    device, args, model, regularization_coeffs, regularization_fns, logger
):
    optimizer = optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    full_data = (
        torch.from_numpy(
            args.data.get_data()[args.data.get_times() != args.leaveout_timepoint]
        )
        .type(torch.float32)
        .to(device)
    )

    best_loss = float("inf")
    

    end = time.time()
    train_loss_fn = SamplesLoss("sinkhorn", p=2, blur=1.0, backend="online")
    for itr in range(1, args.niters + 1):
        model.train()
        optimizer.zero_grad()

        #step the input mapping...take care to account for the regularizedODE wrapper
        odefunc = model.chain[0].odefunc
        if len(regularization_coeffs) > 0:
            odefunc = odefunc.odefunc
        odefunc.diffeq.feature_mapping.step(itr / args.niters)

        # Train
        if args.spectral_norm:
            spectral_norm_power_iteration(model, 1)

        loss = compute_loss(device, args, model, logger, full_data, train_loss_fn)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            # Only regularize on the last timepoint
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff
                for reg_state, coeff in zip(reg_states, regularization_coeffs)
                if coeff != 0
            )
            loss = loss + reg_loss
        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()



        # Eval
        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            "Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) |"
            " NFE Forward {:.0f}({:.1f})"
            " | NFE Backward {:.0f}({:.1f})".format(
                itr,
                time_meter.val,
                time_meter.avg,
                loss_meter.val,
                loss_meter.avg,
                nfef_meter.val,
                nfef_meter.avg,
                nfeb_meter.val,
                nfeb_meter.avg,
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(
                log_message, regularization_fns, reg_states
            )
        
        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                train_eval(
                    device, args, model, itr, best_loss, logger, full_data
                )

        if itr % args.viz_freq == 0:
            with torch.no_grad():
                    visualize(device, args, model, itr)

        if itr % args.save_freq == 0:
            chkpt = {
                "state_dict": model.state_dict(),
            }

            utils.save_checkpoint(
                chkpt,
                args.save,
                epoch=itr,
            )
        end = time.time()
    logger.info("Training has finished.")


def train_eval(device, args, model, itr, best_loss, logger, full_data):
    model.eval()
    test_loss = compute_loss(device, args, model, logger, full_data)
    test_nfe = count_nfe(model)
    log_message = "[TEST] Iter {:04d} | Test Loss {:.6f} |" " NFE {:.0f}".format(
        itr, test_loss, test_nfe
    )
    logger.info(log_message)
    utils.makedirs(args.save)
    with open(os.path.join(args.save, "train_eval.csv"), "a") as f:
        import csv

        writer = csv.writer(f)
        writer.writerow((itr, test_loss))

    if test_loss.item() < best_loss:
        best_loss = test_loss.item()
        chkpt = {
            "state_dict": model.state_dict(),
        }
        torch.save(
            chkpt,
            os.path.join(args.save, "checkpt.pth"),
        )


def visualize(device, args, model, itr):
    model.eval()
    for i, tp in enumerate(args.timepoints):
        idx = args.data.sample_index(args.viz_batch_size, tp)
        p_samples = args.data.get_data()[idx]
        sample_fn, density_fn = get_transforms(
            device, args, model, args.int_tps[: i + 1]
        )
        plt.figure(figsize=(9, 3))
        visualize_transform(
            p_samples,
            args.data.base_sample(),
            args.data.base_density(),
            transform=sample_fn,
            inverse_transform=density_fn,
            samples=True,
            npts=100,
            device=device,
        )
        fig_filename = os.path.join(
            args.save, "figs", "{:04d}_{:01d}.jpg".format(itr, i)
        )
        utils.makedirs(os.path.dirname(fig_filename))
        plt.savefig(fig_filename)
        plt.close()


def plot_output(device, args, model):
    save_traj_dir = os.path.join(args.save, "trajectory")
    # logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = args.data.get_data()[args.data.sample_index(2000, 0)]
    np.random.seed(42)
    start_points = args.data.base_sample()(1000, 2)
    # idx = args.data.sample_index(50, 0)
    # start_points = args.data.get_data()[idx]
    # start_points = torch.from_numpy(start_points).type(torch.float32)
    save_vectors(
        args.data.base_density(),
        model,
        start_points,
        args.data.get_data(),
        args.data.get_times(),
        args.save,
        skip_first=(not args.data.known_base_density()),
        device=device,
        end_times=args.int_tps,
        ntimes=100,
    )
    save_trajectory(
        args.data.base_density(),
        args.data.base_sample(),
        model,
        data_samples,
        save_traj_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
    )
    trajectory_to_video(save_traj_dir)

    density_dir = os.path.join(args.save, "density2")
    save_trajectory_density(
        args.data.base_density(),
        model,
        data_samples,
        density_dir,
        device=device,
        end_times=args.int_tps,
        ntimes=25,
        memory=0.1,
    )
    trajectory_to_video(density_dir)


def main(args):
    # logger
    print(args.no_display_loss)
    utils.makedirs(args.save)

    logger = utils.get_logger(
        logpath=os.path.join(args.save, "logs"),
        filepath=os.path.abspath(__file__),
        displaying=~args.no_display_loss,
    )

    logger.info(args)

    device = torch.device(
        "cuda:" + str(args.gpu) if torch.cuda.is_available() else "cpu"
    )
    if args.use_cpu:
        device = torch.device("cpu")

    args.data = dataset.SCData.factory(args.dataset, args)

    args.timepoints = args.data.get_unique_times()
    # Use maximum timepoint to establish integration_times
    # as some timepoints may be left out for validation etc.
    args.int_tps = (np.arange(max(args.timepoints) + 1) + 1.0) * args.time_scale

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_vanilla(args, args.data.get_shape()[0], regularization_fns).to(
        device
    )

    if args.spectral_norm:
        add_spectral_norm(model)

    logger.info(model)
    n_param = count_parameters(model)
    logger.info("Number of trainable parameters: {}".format(n_param))

    train(
        device,
        args,
        model,
        regularization_coeffs,
        regularization_fns,
        logger,
    )

    if args.data.data.shape[1] == 2:
        plot_output(device, args, model)


if __name__ == "__main__":

    args = parser.parse_args()
    main(args)

/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=262, bias=False)
          )
          (fc1): Linear(in_features=262, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=262, bias=False)
          )
          (fc1): Linear(in_features=262, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=262, bias=False)
          )
          (fc1): Linear(in_features=262, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=262, bias=False)
          )
          (fc1): Linear(in_features=262, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=262, bias=False)
          )
          (fc1): Linear(in_features=262, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=262, bias=False)
          )
          (fc1): Linear(in_features=262, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=262, bias=False)
          )
          (fc1): Linear(in_features=262, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=262, bias=False)
          )
          (fc1): Linear(in_features=262, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=262, bias=False)
          )
          (fc1): Linear(in_features=262, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=262, bias=False)
          )
          (fc1): Linear(in_features=262, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=262, bias=False)
          )
          (fc1): Linear(in_features=262, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=262, bias=False)
          )
          (fc1): Linear(in_features=262, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=262, bias=False)
          )
          (fc1): Linear(in_features=262, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=262, bias=False)
          )
          (fc1): Linear(in_features=262, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 134407
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=306, bias=False)
          )
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=306, bias=False)
          )
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=306, bias=False)
          )
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=306, bias=False)
          )
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=306, bias=False)
          )
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=306, bias=False)
          )
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=306, bias=False)
          )
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=306, bias=False)
          )
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=306, bias=False)
          )
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=306, bias=False)
          )
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping(
            (B): Linear(in_features=6, out_features=306, bias=False)
          )
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.5404(8.5404) | Loss 3.730536(3.730536) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=500, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=100, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.4630(8.4630) | Loss 3.802281(3.802281) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0002 | Time 8.5095(8.4662) | Loss 3.533292(3.783452) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0003 | Time 8.5385(8.4713) | Loss 3.318767(3.750924) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=3, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.4848(8.4848) | Loss 3.807751(3.807751) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0002 | Time 8.4443(8.4820) | Loss 3.448832(3.782627) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0003 | Time 8.4963(8.4830) | Loss 3.124252(3.736541) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=3, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.4804(8.4804) | Loss 3.820882(3.820882) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0002 | Time 8.5013(8.4819) | Loss 3.514707(3.799450) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0003 | Time 8.5357(8.4857) | Loss 3.236134(3.760018) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=3, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.6197(8.6197) | Loss 3.593749(3.593749) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0002 | Time 8.4798(8.6099) | Loss 3.341886(3.576119) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0003 | Time 8.5219(8.6037) | Loss 3.107496(3.543315) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0004 | Time 8.5706(8.6014) | Loss 2.884350(3.497188) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=3, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.6142(8.6142) | Loss 3.629869(3.629869) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0002 | Time 8.5882(8.6124) | Loss 3.381641(3.612493) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0003 | Time 8.4866(8.6036) | Loss 3.149595(3.580091) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, alpha=0.0, atol=1e-05, batch_norm=False, batch_size=1000, bn_lag=0, dataset='EB-PCA', dims='64-64-64', divergence_fn='brute_force', dl2int=None, dtl2int=None, embedding_name='pca', gpu=0, l1int=None, l2int=None, leaveout_timepoint=2, log_freq=10, lr=0.001, max_dim=5, niters=5000, no_display_loss=True, nonlinearity='tanh', num_blocks=1, num_workers=8, rademacher=False, residual=False, rtol=1e-05, save='../results/diego_newloss', save_freq=1000, sl2int=None, solver='dopri5', spectral_norm=False, step_size=None, stochastic=False, test=False, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_scale=0.5, top_k_reg=0.0, train_T=True, training_noise=0.0, use_cpu=False, val_freq=100, viz_batch_size=2000, viz_freq=3, viz_freq_growth=100, weight_decay=1e-05, whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=3, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.5072(8.5072) | Loss 3.811149(3.811149) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0002 | Time 8.4476(8.5030) | Loss 3.536184(3.791901) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0003 | Time 8.5109(8.5036) | Loss 3.285860(3.756478) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=3, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.4473(8.4473) | Loss 3.710420(3.710420) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0002 | Time 8.4353(8.4465) | Loss 3.432656(3.690976) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0003 | Time 8.4880(8.4494) | Loss 3.178075(3.655073) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0004 | Time 8.5419(8.4559) | Loss 2.939087(3.604954) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=3, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.5507(8.5507) | Loss 3.725778(3.725778) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=3, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.5619(8.5619) | Loss 3.814736(3.814736) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0002 | Time 8.5819(8.5633) | Loss 3.458663(3.789811) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0003 | Time 8.5625(8.5632) | Loss 3.134178(3.743917) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=3, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.5896(8.5896) | Loss 3.840712(3.840712) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0002 | Time 8.6006(8.5904) | Loss 3.500947(3.816929) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0003 | Time 8.7697(8.6030) | Loss 3.188854(3.772963) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=3, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.5399(8.5399) | Loss 3.629515(3.629515) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0002 | Time 8.5346(8.5395) | Loss 3.343758(3.609512) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0003 | Time 8.5103(8.5375) | Loss 3.076990(3.572236) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=2, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.5303(8.5303) | Loss 3.678349(3.678349) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0002 | Time 8.5879(8.5343) | Loss 3.419348(3.660219) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=2, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.4907(8.4907) | Loss 3.673192(3.673192) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0002 | Time 8.4480(8.4877) | Loss 3.360121(3.651277) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=2, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.5560(8.5560) | Loss 3.814041(3.814041) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
Iter 0002 | Time 8.5228(8.5536) | Loss 3.449371(3.788514) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=1, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.4462(8.4462) | Loss 3.863832(3.863832) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=1, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.5068(8.5068) | Loss 3.595836(3.595836) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=1, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.5022(8.5022) | Loss 3.873252(3.873252) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=1, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.4849(8.4849) | Loss 3.806284(3.806284) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=1, viz_freq_growth=100, val_freq=100, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.6222(8.6222) | Loss 3.743078(3.743078) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=3, viz_freq_growth=100, val_freq=1, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.4792(8.4792) | Loss 3.808861(3.808861) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=3, viz_freq_growth=100, val_freq=1, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.4243(8.4243) | Loss 3.621200(3.621200) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=3, viz_freq_growth=100, val_freq=1, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.4653(8.4653) | Loss 3.844946(3.844946) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=3, viz_freq_growth=100, val_freq=1, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.4536(8.4536) | Loss 3.874873(3.874873) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=3, viz_freq_growth=100, val_freq=1, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.5348(8.5348) | Loss 3.842413(3.842413) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
[TEST] Iter 0001 | Test Loss 3.562980 | NFE 14 | EMD F/B 1.3763/0.8262
/Users/diegoesc/Dropbox/_academics/college/UROP/trajnet_ablation/SinkhornTrajNet/TrajectoryNet/main.py
Namespace(test=False, dataset='EB-PCA', leaveout_timepoint=2, max_dim=5, dims='64-64-64', num_blocks=1, time_scale=0.5, train_T=True, divergence_fn='brute_force', nonlinearity='tanh', stochastic=False, alpha=0.0, solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, niters=5000, num_workers=8, batch_size=1000, test_batch_size=1000, viz_batch_size=2000, lr=0.001, weight_decay=1e-05, l1int=None, l2int=None, sl2int=None, dl2int=None, dtl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, save='../results/diego_newloss', save_freq=1000, viz_freq=3, viz_freq_growth=100, val_freq=1, log_freq=10, gpu=0, use_cpu=False, no_display_loss=True, top_k_reg=0.0, training_noise=0.0, embedding_name='pca', whiten=False)
SequentialFlow(
  (chain): ModuleList(
    (0): ODEHandler(
      (odefunc): VanillaODEfunc(
        (diffeq): DiffeqNet(
          (time_proj_unactivated): Linear(in_features=1, out_features=1, bias=True)
          (feature_mapping): InputMapping()
          (fc1): Linear(in_features=306, out_features=256, bias=True)
          (fc2): Linear(in_features=256, out_features=256, bias=True)
          (fc3): Linear(in_features=256, out_features=5, bias=True)
          (nonl): Tanh()
        )
      )
    )
  )
)
Number of trainable parameters: 145671
Iter 0001 | Time 8.4592(8.4592) | Loss 3.909400(3.909400) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
[TEST] Iter 0001 | Test Loss 3.598763 | NFE 14 | EMD F/B 1.3723/0.8252
Iter 0002 | Time 8.4974(8.4619) | Loss 3.598763(3.887656) | NFE Forward 14(14.0) | NFE Backward 60(60.0)
